import matplotlib.pyplot as plt
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import torch.nn as nn
import torch
import argparse

parser = argparse.ArgumentParser('test for general adversarial training')
parser.add_argument('--seed', type=int, default=0, help='random seed')
parser.add_argument('--lr', type=float, default=1e-2, help='learning rate')
parser.add_argument('--lr_cnn', type=float, default=1e-1, help='learning rate for cnn model')
parser.add_argument('--batch_size', type=int, default=1, help='batch size for training')
parser.add_argument('--batch_size_rl', type=int, default=1, help='')

parser.add_argument('--epsilon', type=float, default=0.1, help='bounded noise')

parser.add_argument('--num_epoch', type=int, default=3, help='number of training epoch')
parser.add_argument('--num_steps', type=int, default=10, help='training time steps')
parser.add_argument('--start_time', type=int, default=1e3, help='time step to start learning')
parser.add_argument('--start_time_classfier', type=int, default=1e4, help='time step to start learning for the classifier')

parser.add_argument('--z_dim', type=int, default=100, help='dimension of random variable generated by G')
parser.add_argument('--gfv_dim', type=int, default=64, help='dimension of the global feature vector')

parser.add_argument('--action_lim', type=float, default=1., help='action limitation')

parser.add_argument('--dataset', type=str, default='mnist', choices=['mnist', 'fashion-mnist', 'cifar', 'stl10'],
                    help='The name of dataset')
args = parser.parse_args()

if args.dataset == 'cifar' or args.dataset == 'stl10':
    args.channels = 3
    args.state_dim = 32*32
else:
    args.channels = 1
    args.state_dim = 32*32

trans = transforms.Compose([
    transforms.Resize(32),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,)),
])
mnist_train = datasets.MNIST("../data", train=True, download=True, transform=trans)
mnist_test = datasets.MNIST("../data", train=False, download=True, transform=trans)
train_loader = DataLoader(mnist_train, batch_size=args.batch_size, shuffle=True)
test_loader = DataLoader(mnist_test, batch_size=args.batch_size, shuffle=False)

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()

        self.conv_layer = nn.Sequential(

            # Conv Layer block 1
            nn.Conv2d(in_channels=args.channels, out_channels=32, kernel_size=3, padding=1),
            nn.BatchNorm2d(32),
            nn.LeakyReLU(),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),
            nn.LeakyReLU(),
            nn.AvgPool2d(kernel_size=2, stride=2),

            # Conv Layer block 2
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(),
            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),
            nn.LeakyReLU(),
            nn.AvgPool2d(kernel_size=2, stride=2),

            # Conv Layer block 3
            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(),
            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),
            nn.LeakyReLU(),
            nn.AvgPool2d(kernel_size=2, stride=2),
        )

        self.fc_layer = nn.Sequential(
            nn.Linear(4096, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(p=0.2),
            nn.Linear(512, 10)
        )

    def forward(self, x):
        # conv layers
        x = self.conv_layer(x)

        # flatten
        x = x.view(x.size(0), -1)

        # fc layer
        x = self.fc_layer(x)

        return x

import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model_cnn = CNN().to(device)

def plot_images(X,y,yp,M,N):
    f,ax = plt.subplots(M,N, sharex=True, sharey=True, figsize=(N,M*1.3))
    for i in range(M):
        for j in range(N):
            ax[i][j].imshow(1-X[i*N+j][0].cpu().numpy(), cmap="gray")
            title = ax[i][j].set_title("Pred: {}".format(yp[i*N+j].max(dim=0)[1]))
            plt.setp(title, color=('g' if yp[i*N+j].max(dim=0)[1] == y[i*N+j] else 'r'))
            ax[i][j].set_axis_off()
    plt.tight_layout()
    plt.savefig('fig.png')

def plot_images2(X,y,yp,M,N):
    f,ax = plt.subplots(M,N, sharex=True, sharey=True, figsize=(N,M*1.3))
    for i in range(M):
        for j in range(N):
            ax[i][j].imshow(1-X[i*N+j][0].cpu().numpy(), cmap="gray")
            title = ax[i][j].set_title("Pred: {}".format(yp[i*N+j].max(dim=0)[1]))
            plt.setp(title, color=('g' if yp[i*N+j].max(dim=0)[1] == y[i*N+j] else 'r'))
            ax[i][j].set_axis_off()
    plt.tight_layout()
    plt.savefig('fig_adv.png')

# model_cnn.load_state_dict(torch.load("trained_models/model_main_rl_mnist_1.pt"))
model_cnn.load_state_dict(torch.load("trained_models/model-main-rl-mnist-4.pt"))
# model_cnn.load_state_dict(torch.load("trained_models/model_cnn_rl_main4.pt"))

from models.wgan_gradient_penalty import Generator, Discriminator
G = Generator(args.channels).to(device)
D = Discriminator(args.channels).to(device)
G.load_state_dict(torch.load("generator_mnist.pkl"))
D.load_state_dict(torch.load("discriminator_mnist.pkl"))

from ddpg import DDPG
ddpg = DDPG(args.state_dim, args.z_dim, args.action_lim).to(device)
# ddpg.load_state_dict(torch.load("trained_models/DDPG_MINIST_main1.pt"))
# ddpg.load_state_dict(torch.load('trained_models/DDPG_model_rl_mnist_1.pt'))
ddpg.load_state_dict(torch.load('DDPG-model-rl-mnist-4.pt'))
for X, y in test_loader:
    X, y = X.to(device), y.to(device)
    break
yp = model_cnn(X)
# plot_images(X, y, yp, 3, 6)


def to_img(x):
    x = (x + 1.) * 0.5
    x = x.clamp(0, 1)
    x = x.view(x.size(0), 1, 28, 28)
    return x

from torchvision.utils import save_image
from torch.autograd import Variable
save_image(X, 'original_img.png')
img = X.view(args.batch_size, -1)

img = Variable(img).to(device)

train_loader_iterator = iter(test_loader)
for i in range(12):
    try:
        data = next(train_loader_iterator)
    except StopIteration:
        train_loader_iterator = iter(train_loader)
        data = next(train_loader_iterator)
    X, y = data

    img = X.view(1, -1).to(device)

outputs = img.view(img.size(0), 1, 32, 32).cpu().data
save_image(outputs, 'real_rl_img_main2.png')

action = ddpg.choose_action(img)
action = action.unsqueeze(2)
action = action.unsqueeze(3)
fake_img = G(action)

fake_img = fake_img.view(args.batch_size, X.shape[1], X.shape[2], -1)
# decode_img = to_img(fake_img).squeeze()


outputs = fake_img.view(fake_img.size(0), 1, 32, 32).cpu().data
# save_image(outputs+X.cpu().data, 'fake_rl_img_main1.png')
save_image(outputs, 'fake_rl_img_main2.png')